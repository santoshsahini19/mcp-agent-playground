{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from google import genai\n",
    "from google.genai import types # type definitions and configurations\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the api keys\n",
    "serp_api_key = os.getenv(\"SERP_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gemini client\n",
    "google_client = genai.Client(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model context protocols provide a standardized way for different machine learning models and tools to interact and share information, improving collaboration and reusability. Imagine them as agreements defining how models \"talk\" to each other.\n",
      "\n",
      "These protocols standardize data formats, model metadata (like input/output types), and execution methods. This allows tools to easily discover, understand, and utilize different models without needing custom code for each. By defining a common language for models, context protocols enable features like model chaining, automated model selection, and model explainability, fostering a more modular and efficient ML ecosystem. They essentially make models plug-and-play components.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = google_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"Provide a high level overview of model context protocol under 150 words.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mcp libraries\n",
    "from mcp import ClientSession, StdioServerParameters \n",
    "from mcp.client.stdio import stdio_client # to establish connection with mcp server over standard I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure mcp server tool\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"mcp-flight-search\", # points to mcp-flight-search server module \n",
    "    args=[\"--connection_type\", \"stdio\"],\n",
    "    env={\"SERP_API_KEY\": os.getenv(\"SERP_API_KEY\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP server connection and listing tools\n",
    "\n",
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            prompt = f\"Find flights from Raleigh to Miami 2025-06-19\"\n",
    "            \n",
    "            # triggers initial mcp handshake between client and server\n",
    "            await session.initialize() \n",
    "\n",
    "            # requests list of all tools\n",
    "            mcp_tools = await session.list_tools()\n",
    "\n",
    "            tools = [\n",
    "                types.Tool(\n",
    "                    function_declarations=[\n",
    "                        {\n",
    "                            \"name\": tool.name,\n",
    "                            \"description\": tool.description,\n",
    "                            \"parameters\": {\n",
    "                                k: v\n",
    "                                for k,v in tool.inputSchema.items()\n",
    "                                if k not in [\"additionalProperties\", \"$schema\"]\n",
    "                            },\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                for tool in mcp_tools.tools\n",
    "            ]\n",
    "\n",
    "            # connect prompt and tools to gemini flash api\n",
    "            response = google_client.models.generate_content(\n",
    "                model=\"gemini-2.0-flash\",\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0,\n",
    "                    tools=tools\n",
    "                ),\n",
    "            )\n",
    "    #return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('mcp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26283d8beff5d64d963f128c223d709beb597ba2fe460a420c8922b6b1cf8a7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
